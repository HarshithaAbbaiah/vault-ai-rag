# ğŸ”’ VaultAI | Secure Enterprise RAG System

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io)
![Python](https://img.shields.io/badge/Python-3.11-blue?style=flat&logo=python)
![LangChain](https://img.shields.io/badge/Orchestration-LangChain-green?style=flat)
![Llama 3.3](https://img.shields.io/badge/AI_Model-Llama_3.3_70B-purple?style=flat)

> **A privacy-first Retrieval-Augmented Generation (RAG) engine that enables secure, local-first document analysis for enterprise environments.**

---

## ğŸ“– Project Overview
**VaultAI** addresses the critical "Data Privacy Gap" in corporate AI adoption. While public LLMs (like ChatGPT) are powerful, they pose significant security risks for sensitive internal documents (Legal Contracts, HR Policies, Financial Reports).

**VaultAI solves this via a Hybrid Architecture:**
* **Local Storage:** Documents are ingested, chunked, and embedded **locally** on the host machine using `FAISS` and `HuggingFace`. No raw file contents are stored in the cloud.
* **Secure Inference:** We utilize **Groq's LPU Inference Engine** to process *only* the specific retrieved context, ensuring high speed with minimal data exposure.

## âœ¨ Key Features
* **ğŸ“‚ Multi-Document Ingestion:** Capable of processing and indexing multiple PDF files simultaneously.
* **ğŸ§  Hybrid RAG Pipeline:** Combines local vector search (CPU-based FAISS) with cloud-based LLM inference (Llama 3.3).
* **âš¡ Zero-Latency Search:** Optimized vector retrieval for sub-second response times.
* **ğŸ›¡ï¸ Enterprise Grade Privacy:** Adheres to a "Transient Processing" model where data is not used for model training.
* **ğŸ“ Citation-Backed Answers:** Every response includes exact source document references to prevent hallucinations.

## ğŸ› ï¸ Tech Stack
| Component | Technology | Description |
| :--- | :--- | :--- |
| **Frontend** | Streamlit | Reactive web interface for seamless user interaction. |
| **Orchestration** | LangChain | Framework for chaining LLM prompts and retrieval steps. |
| **Vector DB** | FAISS (CPU) | Local, high-efficiency similarity search for dense vectors. |
| **Embeddings** | HuggingFace | `all-MiniLM-L6-v2` model runs locally to vectorize text. |
| **Inference** | Groq API | Ultra-low latency inference for Llama 3.3 (70B). |

## ğŸ—ï¸ System Architecture

```text
                                       User
                                        â”‚
                                        â”‚ 1. Upload Documents (PDF)
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit Application (Local Host)                                     â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚   Ingestion Engine   â”‚          â”‚   Retrieval Engine (RAG)      â”‚   â”‚
â”‚   â”‚                      â”‚          â”‚                               â”‚   â”‚
â”‚   â”‚  â€¢ PyPDF Loader      â”‚          â”‚  1. Semantic Search (FAISS)   â”‚   â”‚
â”‚   â”‚  â€¢ Text Splitter     â”‚          â”‚  2. Context Extraction        â”‚   â”‚
â”‚   â”‚  â€¢ Embeddings Model  â”‚          â”‚  3. Prompt Engineering        â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â”‚                                     â”‚                    â”‚
â”‚              â”‚ (Vectors)                           â”‚ (Relevant Context) â”‚
â”‚              â–¼                                     â”‚                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚                    â”‚
â”‚   â”‚  Local Vector Store  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚   â”‚  (FAISS Index)       â”‚                                              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ 2. Send Prompt + Context
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Groq Cloud API (LPU Inference)                                         â”‚
â”‚                                                                         â”‚
â”‚   â€¢ Model: Llama-3.3-70b-versatile                                      â”‚
â”‚   â€¢ Processing: Zero Data Retention                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ 3. Return AI Answer
               â–¼
          User Screen
